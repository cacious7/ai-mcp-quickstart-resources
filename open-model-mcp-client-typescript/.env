# Configuration for Open Model MCP Client

# Ollama settings
OLLAMA_HOST=http://localhost:11434

# Model Selection - Choose one of:
# - llama3 (8B) - Standard model with good function calling
# - phi3-mini (3.8B) - Fast, lightweight model
# - llama3-instruct (8B) - Instruction-tuned version of Llama 3

# Current model to use
OLLAMA_MODEL=phi4-mini

# Model parameters (adjust for specific models as needed)
MODEL_TEMPERATURE=0.7
MODEL_TOP_P=0.9
